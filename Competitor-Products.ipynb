{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sentiment Data\n",
    "\n",
    "Data (public domain): https://data.world/crowdflower/brands-and-product-emotions\n",
    "\n",
    "Notebook code based on IMDB notebook from bert-sklearn/other_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joep/Desktop/NashAnalytics2019/NAS2019\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from ftfy import fix_text\n",
    " \n",
    "from bert_sklearn import BertClassifier\n",
    "from bert_sklearn import load_model\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "DATAFILE = \"./data/judge-cleaned-up.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "6                                                      \n",
       "7  #SXSW is just starting, #CTIA is around the co...   \n",
       "8  Beautifully smart and simple idea RT @madebyma...   \n",
       "9  Counting down the days to #sxsw plus strong Ca...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "5                             NaN   \n",
       "6                             NaN   \n",
       "7                         Android   \n",
       "8              iPad or iPhone App   \n",
       "9                           Apple   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  \n",
       "5                 No emotion toward brand or product  \n",
       "6                 No emotion toward brand or product  \n",
       "7                                   Positive emotion  \n",
       "8                                   Positive emotion  \n",
       "9                                   Positive emotion  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prep Data\n",
    "\n",
    "def cleanup(txt):\n",
    "    return fix_text(txt)\n",
    "    \n",
    "converters = {'tweet_text': cleanup}\n",
    "    \n",
    "raw_data = pd.read_csv(DATAFILE, converters=converters, encoding='unicode_escape')\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>text</th>\n",
       "      <th>company</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>label</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Android</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "6                                                      \n",
       "7  #SXSW is just starting, #CTIA is around the co...   \n",
       "8  Beautifully smart and simple idea RT @madebyma...   \n",
       "9  Counting down the days to #sxsw plus strong Ca...   \n",
       "\n",
       "                                                text company  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   Apple   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   Apple   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   Apple   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google   \n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...           \n",
       "6                                                              \n",
       "7  #SXSW is just starting, #CTIA is around the co...  Google   \n",
       "8  Beautifully smart and simple idea RT @madebyma...   Apple   \n",
       "9  Counting down the days to #sxsw plus strong Ca...   Apple   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  label  \\\n",
       "0                          iPhone     -1   \n",
       "1              iPad or iPhone App      1   \n",
       "2                            iPad      1   \n",
       "3              iPad or iPhone App     -1   \n",
       "4                          Google      1   \n",
       "5                             NaN      0   \n",
       "6                             NaN      0   \n",
       "7                         Android      1   \n",
       "8              iPad or iPhone App      1   \n",
       "9                           Apple      1   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  \n",
       "5                 No emotion toward brand or product  \n",
       "6                 No emotion toward brand or product  \n",
       "7                                   Positive emotion  \n",
       "8                                   Positive emotion  \n",
       "9                                   Positive emotion  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform columns\n",
    "## ONLY RUN THIS CELL ONCE!!!\n",
    "\n",
    "# Add columns to make the labels usable by the model\n",
    "# tweet_text => text\n",
    "# Positive / No emotion / Negative => 1, 0, -1\n",
    "# Product: Apple stuff, Google stuff, NaN => Apple, Google, ''\n",
    "\n",
    "def clean_text(txt):\n",
    "    return txt\n",
    "raw_data.insert(1, \"text\", np.vectorize(clean_text)(raw_data['tweet_text']))\n",
    "\n",
    "def create_labels(sentiment):\n",
    "    if sentiment.startswith('Positive'):\n",
    "        return 1\n",
    "    if sentiment.startswith('Negative'):\n",
    "        return -1\n",
    "    return 0\n",
    "raw_data.insert(3, 'label', np.vectorize(create_labels)(raw_data['is_there_an_emotion_directed_at_a_brand_or_product']))\n",
    "\n",
    "def get_company(product):\n",
    "    if pd.isnull(product):\n",
    "        return ''\n",
    "    if 'iPad' in product or 'iPhone' in product or 'Apple' in product:\n",
    "        return 'Apple'\n",
    "    if 'Google' in product or 'Android' in product:\n",
    "        return 'Google'\n",
    "    return ''\n",
    "raw_data.insert(2, 'company', np.vectorize(get_company)(raw_data['emotion_in_tweet_is_directed_at']))\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Data Preparation Step\n",
    "# Clean up characters and pull out columns of interest\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)       \n",
    "    return text\n",
    "\n",
    "data = raw_data.filter(['text', 'company', 'label'], axis=1)\n",
    "data['text'] = data['text'].transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (7274, 3)\n",
      "Test data size: (1819, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test data\n",
    "\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "test = data[~msk]\n",
    "print('Training data size: ' + str(train.shape))\n",
    "print('Test data size: ' + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.',\n",
       "        'Apple', -1]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each review is much longer than a sentence or two. The Google AI BERT models were trained on sequences of max length 512. Lets look at the performance for max_seq_length equal to  128, 256, and 512.\n",
    "\n",
    "### max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 800 \n",
      "Test data size: 500 \n"
     ]
    }
   ],
   "source": [
    "## Set up data for the classifier\n",
    "\n",
    "train = train.sample(800)\n",
    "test = test.sample(500)\n",
    "\n",
    "print(\"Train data size: %d \"%(len(train)))\n",
    "print(\"Test data size: %d \"%(len(test)))\n",
    "\n",
    "X_train = train['text']\n",
    "y_train = train['label']\n",
    "\n",
    "X_test = test['text']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n",
      "BertClassifier(bert_config_json=None, bert_model='bert-base-uncased',\n",
      "               bert_vocab=None, do_lower_case=None, epochs=4, eval_batch_size=8,\n",
      "               fp16=False, from_tf=False, gradient_accumulation_steps=1,\n",
      "               ignore_label=None, label_list=[-1, 0, 1], learning_rate=2e-05,\n",
      "               local_rank=-1, logfile='bert_sklearn.log', loss_scale=0,\n",
      "               max_seq_length=128, num_mlp_hiddens=500, num_mlp_layers=0,\n",
      "               random_state=42, restore_file=None, train_batch_size=32,\n",
      "               use_cuda=True, validation_fraction=0.1, warmup_proportion=0.1)\n"
     ]
    }
   ],
   "source": [
    "## Create the model\n",
    "\n",
    "model = BertClassifier(bert_model='bert-base-uncased', label_list=[-1,0,1])\n",
    "model.max_seq_length = 128\n",
    "model.learning_rate = 2e-05\n",
    "model.epochs = 4\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 720, validation data size: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  :   0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "Training  :   0%|          | 0/23 [00:54<?, ?it/s, loss=1.29]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [00:54<19:48, 54.02s/it, loss=1.29]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [01:47<19:48, 54.02s/it, loss=1.27]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [01:47<18:51, 53.90s/it, loss=1.27]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [02:39<18:51, 53.90s/it, loss=1.25]\u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [02:39<17:43, 53.19s/it, loss=1.25]\u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [03:26<17:43, 53.19s/it, loss=1.21]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [03:26<16:17, 51.44s/it, loss=1.21]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [04:21<16:17, 51.44s/it, loss=1.19]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [04:21<15:45, 52.54s/it, loss=1.19]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [05:13<15:45, 52.54s/it, loss=1.17]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [05:13<14:49, 52.33s/it, loss=1.17]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [06:01<14:49, 52.33s/it, loss=1.16]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [06:01<13:35, 50.95s/it, loss=1.16]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [06:54<13:35, 50.95s/it, loss=1.13]\u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [06:54<12:56, 51.77s/it, loss=1.13]\u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [07:43<12:56, 51.77s/it, loss=1.11]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [07:43<11:49, 50.70s/it, loss=1.11]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [08:34<11:49, 50.70s/it, loss=1.08]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [08:34<11:02, 50.98s/it, loss=1.08]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [09:25<11:02, 50.98s/it, loss=1.06]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [09:25<10:09, 50.77s/it, loss=1.06]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [10:12<10:09, 50.77s/it, loss=1.04]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [10:12<09:07, 49.79s/it, loss=1.04]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [11:00<09:07, 49.79s/it, loss=1.04]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [11:00<08:12, 49.27s/it, loss=1.04]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [11:58<08:12, 49.27s/it, loss=1.03]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [11:58<07:47, 51.93s/it, loss=1.03]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [13:00<07:47, 51.93s/it, loss=1]   \u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [13:00<07:18, 54.76s/it, loss=1]\u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [13:51<07:18, 54.76s/it, loss=0.995]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [13:51<06:17, 53.87s/it, loss=0.995]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [14:48<06:17, 53.87s/it, loss=0.98] \u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [14:48<05:27, 54.57s/it, loss=0.98]\u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [15:44<05:27, 54.57s/it, loss=0.964]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [15:44<04:36, 55.23s/it, loss=0.964]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [16:48<04:36, 55.23s/it, loss=0.957]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [16:48<03:50, 57.61s/it, loss=0.957]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [17:50<03:50, 57.61s/it, loss=0.956]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [17:50<02:56, 58.99s/it, loss=0.956]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [18:52<02:56, 58.99s/it, loss=0.959]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [18:52<02:00, 60.11s/it, loss=0.959]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [19:52<02:00, 60.11s/it, loss=0.955]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [19:52<00:59, 59.82s/it, loss=0.955]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [20:14<00:59, 59.82s/it, loss=0.957]\u001b[A\n",
      "Training  : 100%|██████████| 23/23 [20:14<00:00, 48.49s/it, loss=0.957]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  10%|█         | 1/10 [00:03<00:29,  3.32s/it]\u001b[A\n",
      "Validating:  20%|██        | 2/10 [00:06<00:26,  3.32s/it]\u001b[A\n",
      "Validating:  30%|███       | 3/10 [00:10<00:23,  3.37s/it]\u001b[A\n",
      "Validating:  40%|████      | 4/10 [00:13<00:20,  3.40s/it]\u001b[A\n",
      "Validating:  50%|█████     | 5/10 [00:16<00:16,  3.39s/it]\u001b[A\n",
      "Validating:  60%|██████    | 6/10 [00:20<00:13,  3.40s/it]\u001b[A\n",
      "Validating:  70%|███████   | 7/10 [00:23<00:10,  3.34s/it]\u001b[A\n",
      "Validating:  80%|████████  | 8/10 [00:26<00:06,  3.25s/it]\u001b[A\n",
      "Validating:  90%|█████████ | 9/10 [00:29<00:03,  3.18s/it]\u001b[A\n",
      "Validating: 100%|██████████| 10/10 [00:33<00:00,  3.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.9567, Val loss: 0.7739, Val accy: 61.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  :   0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "Training  :   0%|          | 0/23 [00:56<?, ?it/s, loss=0.843]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [00:56<20:46, 56.66s/it, loss=0.843]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [02:03<20:46, 56.66s/it, loss=0.861]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [02:03<20:54, 59.74s/it, loss=0.861]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [03:00<20:54, 59.74s/it, loss=0.837]\u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [03:00<19:35, 58.76s/it, loss=0.837]\u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [03:53<19:35, 58.76s/it, loss=0.839]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [03:53<18:07, 57.26s/it, loss=0.839]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [04:43<18:07, 57.26s/it, loss=0.814]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [04:43<16:27, 54.87s/it, loss=0.814]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [05:36<16:27, 54.87s/it, loss=0.792]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [05:36<15:24, 54.40s/it, loss=0.792]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [06:30<15:24, 54.40s/it, loss=0.806]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [06:30<14:27, 54.24s/it, loss=0.806]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [07:28<14:27, 54.24s/it, loss=0.784]\u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [07:28<13:52, 55.47s/it, loss=0.784]\u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [08:24<13:52, 55.47s/it, loss=0.805]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [08:24<12:59, 55.70s/it, loss=0.805]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [09:19<12:59, 55.70s/it, loss=0.801]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [09:19<11:58, 55.24s/it, loss=0.801]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [10:12<11:58, 55.24s/it, loss=0.816]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [10:12<10:57, 54.76s/it, loss=0.816]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [11:02<10:57, 54.76s/it, loss=0.817]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [11:02<09:46, 53.35s/it, loss=0.817]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [11:54<09:46, 53.35s/it, loss=0.825]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [11:54<08:48, 52.83s/it, loss=0.825]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [12:44<08:48, 52.83s/it, loss=0.819]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [12:44<07:47, 51.90s/it, loss=0.819]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [13:34<07:47, 51.90s/it, loss=0.809]\u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [13:34<06:50, 51.31s/it, loss=0.809]\u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [14:26<06:50, 51.31s/it, loss=0.825]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [14:26<06:00, 51.57s/it, loss=0.825]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [15:15<06:00, 51.57s/it, loss=0.84] \u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [15:15<05:06, 51.03s/it, loss=0.84]\u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [16:09<05:06, 51.03s/it, loss=0.828]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [16:09<04:19, 51.87s/it, loss=0.828]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [17:00<04:19, 51.87s/it, loss=0.823]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [17:00<03:26, 51.67s/it, loss=0.823]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [17:53<03:26, 51.67s/it, loss=0.817]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [17:53<02:35, 51.82s/it, loss=0.817]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [18:45<02:35, 51.82s/it, loss=0.826]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [18:45<01:43, 51.93s/it, loss=0.826]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [19:34<01:43, 51.93s/it, loss=0.826]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [19:34<00:51, 51.22s/it, loss=0.826]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [19:56<00:51, 51.22s/it, loss=0.83] \u001b[A\n",
      "Training  : 100%|██████████| 23/23 [19:56<00:00, 42.30s/it, loss=0.83]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  10%|█         | 1/10 [00:03<00:28,  3.17s/it]\u001b[A\n",
      "Validating:  20%|██        | 2/10 [00:06<00:24,  3.10s/it]\u001b[A\n",
      "Validating:  30%|███       | 3/10 [00:09<00:21,  3.08s/it]\u001b[A\n",
      "Validating:  40%|████      | 4/10 [00:12<00:18,  3.06s/it]\u001b[A\n",
      "Validating:  50%|█████     | 5/10 [00:15<00:15,  3.06s/it]\u001b[A\n",
      "Validating:  60%|██████    | 6/10 [00:18<00:12,  3.05s/it]\u001b[A\n",
      "Validating:  70%|███████   | 7/10 [00:21<00:09,  3.03s/it]\u001b[A\n",
      "Validating:  80%|████████  | 8/10 [00:24<00:06,  3.04s/it]\u001b[A\n",
      "Validating:  90%|█████████ | 9/10 [00:27<00:03,  3.03s/it]\u001b[A\n",
      "Validating: 100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.8302, Val loss: 0.8497, Val accy: 57.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  :   0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "Training  :   0%|          | 0/23 [00:53<?, ?it/s, loss=1.02]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [00:53<19:37, 53.53s/it, loss=1.02]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [01:54<19:37, 53.53s/it, loss=1.03]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [01:54<19:32, 55.83s/it, loss=1.03]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [02:47<19:32, 55.83s/it, loss=0.95]\u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [02:47<18:16, 54.83s/it, loss=0.95]\u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [03:42<18:16, 54.83s/it, loss=0.911]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [03:42<17:23, 54.92s/it, loss=0.911]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [04:35<17:23, 54.92s/it, loss=0.895]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [04:35<16:20, 54.45s/it, loss=0.895]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [05:27<16:20, 54.45s/it, loss=0.885]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [05:27<15:14, 53.79s/it, loss=0.885]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [06:20<15:14, 53.79s/it, loss=0.849]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [06:20<14:12, 53.28s/it, loss=0.849]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [07:11<14:12, 53.28s/it, loss=0.84] \u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [07:11<13:10, 52.69s/it, loss=0.84]\u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [08:09<13:10, 52.69s/it, loss=0.822]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [08:09<12:41, 54.40s/it, loss=0.822]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [09:04<12:41, 54.40s/it, loss=0.818]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [09:04<11:48, 54.48s/it, loss=0.818]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [09:58<11:48, 54.48s/it, loss=0.813]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [09:58<10:53, 54.47s/it, loss=0.813]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [10:48<10:53, 54.47s/it, loss=0.799]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [10:48<09:44, 53.13s/it, loss=0.799]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [11:38<09:44, 53.13s/it, loss=0.816]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [11:38<08:41, 52.16s/it, loss=0.816]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [12:28<08:41, 52.16s/it, loss=0.819]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [12:28<07:42, 51.39s/it, loss=0.819]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [13:17<07:42, 51.39s/it, loss=0.812]\u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [13:17<06:46, 50.85s/it, loss=0.812]\u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [14:07<06:46, 50.85s/it, loss=0.802]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [14:07<05:52, 50.40s/it, loss=0.802]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [15:00<05:52, 50.40s/it, loss=0.799]\u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [15:00<05:06, 51.13s/it, loss=0.799]\u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [15:50<05:06, 51.13s/it, loss=0.806]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [15:50<04:14, 50.95s/it, loss=0.806]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [16:44<04:14, 50.95s/it, loss=0.802]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [16:44<03:27, 51.84s/it, loss=0.802]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [17:34<03:27, 51.84s/it, loss=0.797]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [17:34<02:33, 51.20s/it, loss=0.797]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [18:24<02:33, 51.20s/it, loss=0.793]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [18:24<01:42, 51.01s/it, loss=0.793]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [19:16<01:42, 51.01s/it, loss=0.788]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [19:16<00:51, 51.31s/it, loss=0.788]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [19:38<00:51, 51.31s/it, loss=0.789]\u001b[A\n",
      "Training  : 100%|██████████| 23/23 [19:38<00:00, 42.29s/it, loss=0.789]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  10%|█         | 1/10 [00:03<00:32,  3.56s/it]\u001b[A\n",
      "Validating:  20%|██        | 2/10 [00:06<00:27,  3.46s/it]\u001b[A\n",
      "Validating:  30%|███       | 3/10 [00:09<00:23,  3.34s/it]\u001b[A\n",
      "Validating:  40%|████      | 4/10 [00:12<00:19,  3.26s/it]\u001b[A\n",
      "Validating:  50%|█████     | 5/10 [00:16<00:17,  3.45s/it]\u001b[A\n",
      "Validating:  60%|██████    | 6/10 [00:20<00:13,  3.45s/it]\u001b[A\n",
      "Validating:  70%|███████   | 7/10 [00:23<00:09,  3.32s/it]\u001b[A\n",
      "Validating:  80%|████████  | 8/10 [00:26<00:06,  3.24s/it]\u001b[A\n",
      "Validating:  90%|█████████ | 9/10 [00:29<00:03,  3.30s/it]\u001b[A\n",
      "Validating: 100%|██████████| 10/10 [00:32<00:00,  3.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.7894, Val loss: 0.8008, Val accy: 56.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  :   0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "Training  :   0%|          | 0/23 [00:53<?, ?it/s, loss=0.643]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [00:53<19:46, 53.93s/it, loss=0.643]\u001b[A\n",
      "Training  :   4%|▍         | 1/23 [01:54<19:46, 53.93s/it, loss=0.653]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [01:54<19:32, 55.82s/it, loss=0.653]\u001b[A\n",
      "Training  :   9%|▊         | 2/23 [02:49<19:32, 55.82s/it, loss=0.63] \u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [02:49<18:31, 55.58s/it, loss=0.63]\u001b[A\n",
      "Training  :  13%|█▎        | 3/23 [03:39<18:31, 55.58s/it, loss=0.717]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [03:39<17:05, 53.99s/it, loss=0.717]\u001b[A\n",
      "Training  :  17%|█▋        | 4/23 [04:31<17:05, 53.99s/it, loss=0.692]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [04:31<16:01, 53.43s/it, loss=0.692]\u001b[A\n",
      "Training  :  22%|██▏       | 5/23 [05:26<16:01, 53.43s/it, loss=0.677]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [05:26<15:15, 53.84s/it, loss=0.677]\u001b[A\n",
      "Training  :  26%|██▌       | 6/23 [06:17<15:15, 53.84s/it, loss=0.669]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [06:17<14:07, 52.99s/it, loss=0.669]\u001b[A\n",
      "Training  :  30%|███       | 7/23 [07:07<14:07, 52.99s/it, loss=0.664]\u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [07:07<13:02, 52.15s/it, loss=0.664]\u001b[A\n",
      "Training  :  35%|███▍      | 8/23 [07:56<13:02, 52.15s/it, loss=0.663]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [07:56<11:56, 51.18s/it, loss=0.663]\u001b[A\n",
      "Training  :  39%|███▉      | 9/23 [08:49<11:56, 51.18s/it, loss=0.655]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [08:49<11:12, 51.74s/it, loss=0.655]\u001b[A\n",
      "Training  :  43%|████▎     | 10/23 [09:41<11:12, 51.74s/it, loss=0.652]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [09:41<10:21, 51.83s/it, loss=0.652]\u001b[A\n",
      "Training  :  48%|████▊     | 11/23 [10:35<10:21, 51.83s/it, loss=0.648]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [10:35<09:35, 52.35s/it, loss=0.648]\u001b[A\n",
      "Training  :  52%|█████▏    | 12/23 [11:30<09:35, 52.35s/it, loss=0.646]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [11:30<08:53, 53.32s/it, loss=0.646]\u001b[A\n",
      "Training  :  57%|█████▋    | 13/23 [12:19<08:53, 53.32s/it, loss=0.645]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [12:19<07:47, 51.95s/it, loss=0.645]\u001b[A\n",
      "Training  :  61%|██████    | 14/23 [13:06<07:47, 51.95s/it, loss=0.658]\u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [13:06<06:44, 50.58s/it, loss=0.658]\u001b[A\n",
      "Training  :  65%|██████▌   | 15/23 [13:58<06:44, 50.58s/it, loss=0.656]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [13:58<05:55, 50.84s/it, loss=0.656]\u001b[A\n",
      "Training  :  70%|██████▉   | 16/23 [14:47<05:55, 50.84s/it, loss=0.661]\u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [14:47<05:02, 50.38s/it, loss=0.661]\u001b[A\n",
      "Training  :  74%|███████▍  | 17/23 [15:38<05:02, 50.38s/it, loss=0.676]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [15:38<04:13, 50.63s/it, loss=0.676]\u001b[A\n",
      "Training  :  78%|███████▊  | 18/23 [16:28<04:13, 50.63s/it, loss=0.679]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [16:28<03:21, 50.44s/it, loss=0.679]\u001b[A\n",
      "Training  :  83%|████████▎ | 19/23 [17:18<03:21, 50.44s/it, loss=0.689]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [17:18<02:30, 50.18s/it, loss=0.689]\u001b[A\n",
      "Training  :  87%|████████▋ | 20/23 [18:06<02:30, 50.18s/it, loss=0.691]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [18:06<01:38, 49.48s/it, loss=0.691]\u001b[A\n",
      "Training  :  91%|█████████▏| 21/23 [18:54<01:38, 49.48s/it, loss=0.698]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [18:54<00:49, 49.10s/it, loss=0.698]\u001b[A\n",
      "Training  :  96%|█████████▌| 22/23 [19:15<00:49, 49.10s/it, loss=0.702]\u001b[A\n",
      "Training  : 100%|██████████| 23/23 [19:15<00:00, 40.62s/it, loss=0.702]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  10%|█         | 1/10 [00:03<00:28,  3.12s/it]\u001b[A\n",
      "Validating:  20%|██        | 2/10 [00:06<00:24,  3.06s/it]\u001b[A\n",
      "Validating:  30%|███       | 3/10 [00:08<00:21,  3.00s/it]\u001b[A\n",
      "Validating:  40%|████      | 4/10 [00:11<00:17,  2.99s/it]\u001b[A\n",
      "Validating:  50%|█████     | 5/10 [00:14<00:14,  2.98s/it]\u001b[A\n",
      "Validating:  60%|██████    | 6/10 [00:17<00:11,  2.96s/it]\u001b[A\n",
      "Validating:  70%|███████   | 7/10 [00:20<00:08,  2.98s/it]\u001b[A\n",
      "Validating:  80%|████████  | 8/10 [00:23<00:05,  2.95s/it]\u001b[A\n",
      "Validating:  90%|█████████ | 9/10 [00:26<00:02,  2.93s/it]\u001b[A\n",
      "Validating: 100%|██████████| 10/10 [00:29<00:00,  2.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 0.7023, Val loss: 0.7511, Val accy: 65.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Testing:   2%|▏         | 1/63 [00:03<03:17,  3.18s/it]\u001b[A\n",
      "Testing:   3%|▎         | 2/63 [00:06<03:09,  3.10s/it]\u001b[A\n",
      "Testing:   5%|▍         | 3/63 [00:08<03:02,  3.04s/it]\u001b[A\n",
      "Testing:   6%|▋         | 4/63 [00:11<02:56,  2.99s/it]\u001b[A\n",
      "Testing:   8%|▊         | 5/63 [00:14<02:52,  2.97s/it]\u001b[A\n",
      "Testing:  10%|▉         | 6/63 [00:18<02:59,  3.15s/it]\u001b[A\n",
      "Testing:  11%|█         | 7/63 [00:21<02:53,  3.09s/it]\u001b[A\n",
      "Testing:  13%|█▎        | 8/63 [00:24<02:51,  3.12s/it]\u001b[A\n",
      "Testing:  14%|█▍        | 9/63 [00:27<02:45,  3.07s/it]\u001b[A\n",
      "Testing:  16%|█▌        | 10/63 [00:30<02:39,  3.01s/it]\u001b[A\n",
      "Testing:  17%|█▋        | 11/63 [00:33<02:34,  2.98s/it]\u001b[A\n",
      "Testing:  19%|█▉        | 12/63 [00:36<02:30,  2.95s/it]\u001b[A\n",
      "Testing:  21%|██        | 13/63 [00:38<02:26,  2.93s/it]\u001b[A\n",
      "Testing:  22%|██▏       | 14/63 [00:41<02:23,  2.92s/it]\u001b[A\n",
      "Testing:  24%|██▍       | 15/63 [00:44<02:22,  2.96s/it]\u001b[A\n",
      "Testing:  25%|██▌       | 16/63 [00:48<02:21,  3.00s/it]\u001b[A\n",
      "Testing:  27%|██▋       | 17/63 [00:50<02:16,  2.97s/it]\u001b[A\n",
      "Testing:  29%|██▊       | 18/63 [00:53<02:13,  2.96s/it]\u001b[A\n",
      "Testing:  30%|███       | 19/63 [00:56<02:09,  2.94s/it]\u001b[A\n",
      "Testing:  32%|███▏      | 20/63 [00:59<02:05,  2.92s/it]\u001b[A\n",
      "Testing:  33%|███▎      | 21/63 [01:02<02:07,  3.03s/it]\u001b[A\n",
      "Testing:  35%|███▍      | 22/63 [01:06<02:05,  3.05s/it]\u001b[A\n",
      "Testing:  37%|███▋      | 23/63 [01:08<01:59,  2.99s/it]\u001b[A\n",
      "Testing:  38%|███▊      | 24/63 [01:11<01:55,  2.97s/it]\u001b[A\n",
      "Testing:  40%|███▉      | 25/63 [01:14<01:52,  2.97s/it]\u001b[A\n",
      "Testing:  41%|████▏     | 26/63 [01:17<01:49,  2.97s/it]\u001b[A\n",
      "Testing:  43%|████▎     | 27/63 [01:20<01:46,  2.96s/it]\u001b[A\n",
      "Testing:  44%|████▍     | 28/63 [01:23<01:42,  2.94s/it]\u001b[A\n",
      "Testing:  46%|████▌     | 29/63 [01:26<01:39,  2.93s/it]\u001b[A\n",
      "Testing:  48%|████▊     | 30/63 [01:29<01:36,  2.92s/it]\u001b[A\n",
      "Testing:  49%|████▉     | 31/63 [01:32<01:33,  2.91s/it]\u001b[A\n",
      "Testing:  51%|█████     | 32/63 [01:35<01:29,  2.90s/it]\u001b[A\n",
      "Testing:  52%|█████▏    | 33/63 [01:38<01:27,  2.90s/it]\u001b[A\n",
      "Testing:  54%|█████▍    | 34/63 [01:41<01:24,  2.91s/it]\u001b[A\n",
      "Testing:  56%|█████▌    | 35/63 [01:43<01:21,  2.91s/it]\u001b[A\n",
      "Testing:  57%|█████▋    | 36/63 [01:46<01:18,  2.92s/it]\u001b[A\n",
      "Testing:  59%|█████▊    | 37/63 [01:49<01:16,  2.93s/it]\u001b[A\n",
      "Testing:  60%|██████    | 38/63 [01:52<01:12,  2.92s/it]\u001b[A\n",
      "Testing:  62%|██████▏   | 39/63 [01:55<01:09,  2.91s/it]\u001b[A\n",
      "Testing:  63%|██████▎   | 40/63 [01:58<01:09,  3.02s/it]\u001b[A\n",
      "Testing:  65%|██████▌   | 41/63 [02:01<01:05,  2.99s/it]\u001b[A\n",
      "Testing:  67%|██████▋   | 42/63 [02:04<01:02,  2.96s/it]\u001b[A\n",
      "Testing:  68%|██████▊   | 43/63 [02:07<00:58,  2.95s/it]\u001b[A\n",
      "Testing:  70%|██████▉   | 44/63 [02:10<00:55,  2.93s/it]\u001b[A\n",
      "Testing:  71%|███████▏  | 45/63 [02:13<00:52,  2.92s/it]\u001b[A\n",
      "Testing:  73%|███████▎  | 46/63 [02:16<00:49,  2.92s/it]\u001b[A\n",
      "Testing:  75%|███████▍  | 47/63 [02:19<00:46,  2.92s/it]\u001b[A\n",
      "Testing:  76%|███████▌  | 48/63 [02:22<00:43,  2.91s/it]\u001b[A\n",
      "Testing:  78%|███████▊  | 49/63 [02:24<00:40,  2.90s/it]\u001b[A\n",
      "Testing:  79%|███████▉  | 50/63 [02:27<00:37,  2.90s/it]\u001b[A\n",
      "Testing:  81%|████████  | 51/63 [02:30<00:34,  2.89s/it]\u001b[A\n",
      "Testing:  83%|████████▎ | 52/63 [02:33<00:31,  2.90s/it]\u001b[A\n",
      "Testing:  84%|████████▍ | 53/63 [02:36<00:29,  2.90s/it]\u001b[A\n",
      "Testing:  86%|████████▌ | 54/63 [02:39<00:26,  2.91s/it]\u001b[A\n",
      "Testing:  87%|████████▋ | 55/63 [02:42<00:23,  2.90s/it]\u001b[A\n",
      "Testing:  89%|████████▉ | 56/63 [02:45<00:20,  2.91s/it]\u001b[A\n",
      "Testing:  90%|█████████ | 57/63 [02:48<00:17,  2.91s/it]\u001b[A\n",
      "Testing:  92%|█████████▏| 58/63 [02:51<00:14,  2.91s/it]\u001b[A\n",
      "Testing:  94%|█████████▎| 59/63 [02:54<00:11,  2.92s/it]\u001b[A\n",
      "Testing:  95%|█████████▌| 60/63 [02:56<00:08,  2.90s/it]\u001b[A\n",
      "Testing:  97%|█████████▋| 61/63 [02:59<00:05,  2.89s/it]\u001b[A\n",
      "Testing:  98%|█████████▊| 62/63 [03:02<00:02,  2.89s/it]\u001b[A\n",
      "Testing: 100%|██████████| 63/63 [03:04<00:00,  2.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.8682, Accuracy: 60.40%\n",
      "CPU times: user 2h 5min 2s, sys: 18min 23s, total: 2h 23min 25s\n",
      "Wall time: 1h 24min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Train the model using our data (this could take a while)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accy = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Predicting: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21474382 0.08676378 0.6984924 ]\n",
      " [0.06963811 0.17175971 0.7586022 ]\n",
      " [0.05299675 0.7281567  0.21884665]\n",
      " [0.07872742 0.5267376  0.39453495]\n",
      " [0.04716099 0.6724587  0.2803803 ]]\n",
      "CPU times: user 3.5 s, sys: 330 ms, total: 3.83 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Test out the model with our own invented examples!\n",
    "\n",
    "examples = [\n",
    "    'This Android product is not very good',\n",
    "    'I could not get that iPhone to work, so I sent it back. I''m really upset!',\n",
    "    'Another great product from the folks at Google!  We really liked it a lot',\n",
    "    'My iPad is essential - of course I would buy another one!','\n",
    "    'When in the course of human events it becomes necessary to dissolve those ties...',\n",
    "    'We the people, in order to form a more perfect union, establish justice, insure domestic tranquility, ...'\n",
    "]\n",
    "\n",
    "print(model.predict_proba(examples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/model1_128_bb_uncased.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Don't use this one - it will take a very long time!\n",
    "\n",
    "model = BertClassifier(bert_model='bert-base-uncased', label_list=[-1,0,1])\n",
    "model.max_seq_length = 256\n",
    "model.train_batch_size = 32\n",
    "model.learning_rate = 2e-05\n",
    "model.epochs = 4\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accy = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Don't use this one - it will take the longest of all!\n",
    "\n",
    "model = BertClassifier(bert_model='bert-base-uncased', label_list=[-1,0,1])\n",
    "model.max_seq_length = 512\n",
    "\n",
    "# max_seq_length=512 will use a lot more GPU mem, so I am turning down batch size \n",
    "# and adding gradient accumulation steps\n",
    "model.train_batch_size = 16\n",
    "model_gradient_accumulation_steps = 4\n",
    "\n",
    "model.learning_rate = 2e-05\n",
    "model.epochs = 4\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accy = model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
